@incollection{honkela_stacked_2011,
	address = {Berlin, Heidelberg},
	title = {Stacked {Convolutional} {Auto}-{Encoders} for {Hierarchical} {Feature} {Extraction}},
	volume = {6791},
	isbn = {978-3-642-21734-0 978-3-642-21735-7},
	url = {http://link.springer.com/10.1007/978-3-642-21735-7_7},
	urldate = {2017-09-27},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2011},
	publisher = {Springer Berlin Heidelberg},
	author = {Masci, Jonathan and Meier, Ueli and Cireşan, Dan and Schmidhuber, Jürgen},
	editor = {Honkela, Timo and Duch, Włodzisław and Girolami, Mark and Kaski, Samuel},
	year = {2011},
	note = {DOI: 10.1007/978-3-642-21735-7\_7},
	pages = {52--59}
}

@article{burbank_mirrored_2015,
	title = {Mirrored {STDP} {Implements} {Autoencoder} {Learning} in a {Network} of {Spiking} {Neurons}},
	volume = {11},
	issn = {1553-7358},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1004566},
	doi = {10.1371/journal.pcbi.1004566},
	language = {en},
	number = {12},
	urldate = {2017-09-27},
	journal = {PLOS Computational Biology},
	author = {Burbank, Kendra S.},
	editor = {Graham, Lyle J.},
	month = dec,
	year = {2015},
	pages = {e1004566}
}

@article{wilamowski_neural_2010,
	title = {Neural {Network} {Learning} {Without} {Backpropagation}},
	volume = {21},
	issn = {1045-9227, 1941-0093},
	url = {http://ieeexplore.ieee.org/document/5580116/},
	doi = {10.1109/TNN.2010.2073482},
	number = {11},
	urldate = {2017-09-27},
	journal = {IEEE Transactions on Neural Networks},
	author = {Wilamowski, B M and {Hao Yu}},
	month = nov,
	year = {2010},
	pages = {1793--1803}
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {00010782},
	url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
	doi = {10.1145/3065386},
	language = {en},
	number = {6},
	urldate = {2017-09-28},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	pages = {84--90}
}

@inproceedings{simard_best_2003,
	title = {Best practices for convolutional neural networks applied to visual document analysis},
	volume = {1},
	isbn = {978-0-7695-1960-9},
	url = {http://ieeexplore.ieee.org/document/1227801/},
	doi = {10.1109/ICDAR.2003.1227801},
	urldate = {2017-09-28},
	publisher = {IEEE Comput. Soc},
	author = {Simard, P.Y. and Steinkraus, D. and Platt, J.C.},
	year = {2003},
	pages = {958--963}
}

@inproceedings{ciresan_convolutional_2011,
	title = {Convolutional {Neural} {Network} {Committees} for {Handwritten} {Character} {Classification}},
	isbn = {978-1-4577-1350-7},
	url = {http://ieeexplore.ieee.org/document/6065487/},
	doi = {10.1109/ICDAR.2011.229},
	urldate = {2017-09-28},
	publisher = {IEEE},
	author = {Ciresan, Dan Claudiu and Meier, Ueli and Gambardella, Luca Maria and Schmidhuber, Jurgen},
	month = sep,
	year = {2011},
	pages = {1135--1139}
}

@inproceedings{karpathy_large-scale_2014,
	title = {Large-{Scale} {Video} {Classification} with {Convolutional} {Neural} {Networks}},
	isbn = {978-1-4799-5118-5},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909619},
	doi = {10.1109/CVPR.2014.223},
	urldate = {2017-09-29},
	publisher = {IEEE},
	author = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
	month = jun,
	year = {2014},
	pages = {1725--1732}
}

@incollection{hinton_recognize_2007,
	title = {To recognize shapes, first learn to generate images},
	volume = {165},
	isbn = {978-0-444-52823-0},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0079612306650346},
	language = {en},
	urldate = {2017-09-29},
	booktitle = {Progress in {Brain} {Research}},
	publisher = {Elsevier},
	author = {Hinton, Geoffrey E.},
	year = {2007},
	note = {DOI: 10.1016/S0079-6123(06)65034-6},
	pages = {535--547}
}

@inproceedings{oquab_learning_2014,
	title = {Learning and {Transferring} {Mid}-level {Image} {Representations} {Using} {Convolutional} {Neural} {Networks}},
	isbn = {978-1-4799-5118-5},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909618},
	doi = {10.1109/CVPR.2014.222},
	urldate = {2017-09-29},
	publisher = {IEEE},
	author = {Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
	month = jun,
	year = {2014},
	pages = {1717--1724}
}

@article{ji_3d_2013,
	title = {3D convolutional neural networks for human action recognition},
	volume = {35},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2012.59},
	abstract = {We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods.},
	language = {eng},
	number = {1},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Ji, Shuiwang and Yang, Ming and Yu, Kai},
	month = jan,
	year = {2013},
	pmid = {22392705},
	keywords = {Algorithms, Decision Support Techniques, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, Movement, Neural Networks (Computer), Pattern Recognition, Automated, Subtraction Technique},
	pages = {221--231}
}

@book{burbank_understanding_2011,
	address = {Cambridge, MA},
	title = {Understanding {Visual} {Population} {Codes}},
	publisher = {MIT Press},
	author = {Burbank and Kreiman, G.},
	year = {2011},
	file = {Introduction to the Anatomy and Function of Visual Cortex | Kreiman Laboratory:C\:\\Users\\Veblen\\Zotero\\storage\\AIGNHFGI\\introduction-anatomy-and-function-visual-cortex.html:text/html}
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {0018-9219},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	keywords = {2D shape variability, back-propagation, backpropagation, Character recognition, cheque reading, complex decision surface synthesis, convolution, convolutional neural network character recognizers, document recognition, document recognition systems, Feature extraction, field extraction, gradient based learning technique, gradient-based learning, graph transformer networks, GTN, handwritten character recognition, handwritten digit recognition task, Hidden Markov models, high-dimensional patterns, language modeling, Machine learning, Multi-layer neural network, multilayer neural networks, multilayer perceptrons, multimodule systems, Neural networks, optical character recognition, Optical character recognition software, Optical computing, Pattern recognition, performance measure minimization, Principal component analysis, segmentation recognition},
	pages = {2278--2324},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Veblen\\Zotero\\storage\\ZG7P75B3\\726791.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Veblen\\Zotero\\storage\\LNXCIG52\\Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf:application/pdf}
}

@inproceedings{vincent_extracting_2008,
	address = {New York, NY, USA},
	series = {{ICML} '08},
	title = {Extracting and {Composing} {Robust} {Features} with {Denoising} {Autoencoders}},
	isbn = {978-1-60558-205-4},
	url = {http://doi.acm.org/10.1145/1390156.1390294},
	doi = {10.1145/1390156.1390294},
	abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
	urldate = {2017-10-03},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	year = {2008},
	pages = {1096--1103},
	file = {ACM Full Text PDF:C\:\\Users\\Veblen\\Zotero\\storage\\38CYAXPY\\Vincent et al. - 2008 - Extracting and Composing Robust Features with Deno.pdf:application/pdf}
}

@inproceedings{socher_semi-supervised_2011,
	address = {Stroudsburg, PA, USA},
	series = {{EMNLP} '11},
	title = {Semi-supervised {Recursive} {Autoencoders} for {Predicting} {Sentiment} {Distributions}},
	isbn = {978-1-937284-11-4},
	url = {http://dl.acm.org/citation.cfm?id=2145432.2145450},
	abstract = {We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model's ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.},
	urldate = {2017-10-03},
	booktitle = {Proceedings of the {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Socher, Richard and Pennington, Jeffrey and Huang, Eric H. and Ng, Andrew Y. and Manning, Christopher D.},
	year = {2011},
	pages = {151--161},
	file = {ACM Full Text PDF:C\:\\Users\\Veblen\\Zotero\\storage\\NADPNPS8\\Socher et al. - 2011 - Semi-supervised Recursive Autoencoders for Predict.pdf:application/pdf}
}

@inproceedings{chen_page_2015,
	title = {Page segmentation of historical document images with convolutional autoencoders},
	doi = {10.1109/ICDAR.2015.7333914},
	abstract = {In this paper, we present an unsupervised feature learning method for page segmentation of historical handwritten documents available as color images. We consider page segmentation as a pixel labeling problem, i.e., each pixel is classified as either periphery, background, text block, or decoration. Traditional methods in this area rely on carefully hand-crafted features or large amounts of prior knowledge. In contrast, we apply convolutional autoencoders to learn features directly from pixel intensity values. Then, using these features to train an SVM, we achieve high quality segmentation without any assumption of specific topologies and shapes. Experiments on three public datasets demonstrate the effectiveness and superiority of the proposed approach.},
	booktitle = {2015 13th {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Chen, K. and Seuret, M. and Liwicki, M. and Hennebert, J. and Ingold, R.},
	month = aug,
	year = {2015},
	keywords = {handwritten character recognition, color images, convolutional autoencoders, document image processing, historical document images, historical handwritten documents, history, image colour analysis, image segmentation, Image segmentation, page segmentation, pixel intensity values, pixel labeling problem, Robustness, support vector machine, support vector machines, Support vector machines, SVM, unsupervised feature learning method, unsupervised learning},
	pages = {1011--1015},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Veblen\\Zotero\\storage\\3DK775W3\\7333914.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Veblen\\Zotero\\storage\\SDZPZIII\\Chen et al. - 2015 - Page segmentation of historical document images wi.pdf:application/pdf}
}

@article{izhikevich_simple_2003,
	title = {Simple model of spiking neurons},
	volume = {14},
	issn = {1045-9227},
	doi = {10.1109/TNN.2003.820440},
	abstract = {A model is presented that reproduces spiking and bursting behavior of known types of cortical neurons. The model combines the biologically plausibility of Hodgkin-Huxley-type dynamics and the computational efficiency of integrate-and-fire neurons. Using this model, one can simulate tens of thousands of spiking cortical neurons in real time (1 ms resolution) using a desktop PC.},
	number = {6},
	journal = {IEEE Transactions on Neural Networks},
	author = {Izhikevich, E. M.},
	month = nov,
	year = {2003},
	keywords = {Bifurcation, Biological system modeling, biologically plausibility, Biology computing, Biomembranes, Brain modeling, brain models, bursting neurons, Computational modeling, cortical neurons, Hodgkin-Huxley-type dynamics, Large-scale systems, Mathematical analysis, Mathematical model, neural nets, Neurons, neurophysiology, quadratic integrate-and-fire neurons, spiking neurons, thalamus},
	pages = {1569--1572},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Veblen\\Zotero\\storage\\384V69VQ\\1257420.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Veblen\\Zotero\\storage\\9BTXLCHK\\Izhikevich - 2003 - Simple model of spiking neurons.pdf:application/pdf}
}

@article{mazzoni_more_1991,
	title = {A more biologically plausible learning rule for neural networks.},
	volume = {88},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/88/10/4433},
	doi = {10.1073/pnas.88.10.4433},
	abstract = {Many recent studies have used artificial neural network algorithms to model how the brain might process information. However, back-propagation learning, the method that is generally used to train these networks, is distinctly "unbiological." We describe here a more biologically plausible learning rule, using reinforcement learning, which we have applied to the problem of how area 7a in the posterior parietal cortex of monkeys might represent visual space in head-centered coordinates. The network behaves similarly to networks trained by using back-propagation and to neurons recorded in area 7a. These results show that a neural network does not require back propagation to acquire biologically interesting properties.},
	language = {en},
	number = {10},
	urldate = {2017-10-03},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Mazzoni, P. and Andersen, R. A. and Jordan, M. I.},
	month = may,
	year = {1991},
	pmid = {1903542},
	pages = {4433--4437},
	file = {Full Text PDF:C\:\\Users\\Veblen\\Zotero\\storage\\VM2R5CRJ\\Mazzoni et al. - 1991 - A more biologically plausible learning rule for ne.pdf:application/pdf;Snapshot:C\:\\Users\\Veblen\\Zotero\\storage\\QEBCRCCZ\\4433.html:text/html}
}

@article{bengio_towards_2015,
	title = {Towards {Biologically} {Plausible} {Deep} {Learning}},
	url = {http://arxiv.org/abs/1502.04156},
	abstract = {Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) arises out of a simple update rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.},
	urldate = {2017-10-03},
	journal = {arXiv:1502.04156 [cs]},
	author = {Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and Mesnard, Thomas and Lin, Zhouhan},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.04156},
	keywords = {Computer Science - Learning},
	file = {arXiv\:1502.04156 PDF:C\:\\Users\\Veblen\\Zotero\\storage\\TAFRL8TI\\Bengio et al. - 2015 - Towards Biologically Plausible Deep Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Veblen\\Zotero\\storage\\AWMTX226\\1502.html:text/html}
}

@incollection{fukushima_neocognitron:_1982,
	series = {Lecture {Notes} in {Biomathematics}},
	title = {Neocognitron: {A} {Self}-{Organizing} {Neural} {Network} {Model} for a {Mechanism} of {Visual} {Pattern} {Recognition}},
	isbn = {978-3-540-11574-8 978-3-642-46466-9},
	shorttitle = {Neocognitron},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-46466-9_18},
	abstract = {A neural network model, called a “neocognitron”, is proposed for a mechanism of visual pattern recognition. It is demonstrated by computer simulation that the neocognitron has characteristics similar to those of visual systems of vertebrates.The neocognitron is a multilayered network consisting of a cascade connection of many layers of cells, and the efficiencies of the synaptic connections between cells are modifiable. Self-organization of the network progresses by means of “learning-without-a-teacher” process: Only repetitive presentation of a set of stimulus patterns is necessary for the self-organization of the network, and no information about the categories to which these patterns should be classified is needed. The neocognitron by itself acquires the ability to classify and correctly recognize these patterns according to the differences in their shapes: Any patterns which we human beings judge to be alike are also judged to be of the same category by the neocognitron. The neocognitron recognizes stimulus patterns correctly without being affected by shifts in position or even by considerable distortions in shape of the stimulus patterns. If a stimulus pattern is presented at a different position or if the shape of the pattern is distorted, the responses of the cells in the intermediate layers, especially the ones near the input layer, vary with the shift in position or the distortion in shape of the pattern. However, the deeper the layer is, the smaller become the variations in cellular responses. Thus, the cells of the deepest layer of the network are not affected by the shift in position or the distortion in shape of the stimulus pattern.},
	language = {en},
	urldate = {2017-10-04},
	booktitle = {Competition and {Cooperation} in {Neural} {Nets}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Fukushima, Kunihiko and Miyake, Sei},
	year = {1982},
	note = {DOI: 10.1007/978-3-642-46466-9\_18},
	pages = {267--285},
	file = {Snapshot:C\:\\Users\\Veblen\\Zotero\\storage\\LNM8NJLY\\978-3-642-46466-9_18.html:text/html}
}

@inproceedings{masci_stacked_2011,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Stacked {Convolutional} {Auto}-{Encoders} for {Hierarchical} {Feature} {Extraction}},
	isbn = {978-3-642-21734-0 978-3-642-21735-7},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-21735-7_7},
	doi = {10.1007/978-3-642-21735-7_7},
	abstract = {We present a novel convolutional auto-encoder (CAE) for unsupervised feature learning. A stack of CAEs forms a convolutional neural network (CNN). Each CAE is trained using conventional on-line gradient descent without additional regularization terms. A max-pooling layer is essential to learn biologically plausible features consistent with those found by previous approaches. Initializing a CNN with filters of a trained CAE stack yields superior performance on a digit (MNIST) and an object recognition (CIFAR10) benchmark.},
	language = {en},
	urldate = {2017-10-04},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2011},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Masci, Jonathan and Meier, Ueli and Cireşan, Dan and Schmidhuber, Jürgen},
	month = jun,
	year = {2011},
	pages = {52--59},
	file = {Full Text PDF:C\:\\Users\\Veblen\\Zotero\\storage\\GBMX3N5L\\Masci et al. - 2011 - Stacked Convolutional Auto-Encoders for Hierarchic.pdf:application/pdf;Snapshot:C\:\\Users\\Veblen\\Zotero\\storage\\NDI3IPTP\\10.html:text/html}
}

@article{vincent_stacked_2010,
	title = {Stacked {Denoising} {Autoencoders}: {Learning} {Useful} {Representations} in a {Deep} {Network} with a {Local} {Denoising} {Criterion}},
	volume = {11},
	issn = {ISSN 1533-7928},
	shorttitle = {Stacked {Denoising} {Autoencoders}},
	url = {http://www.jmlr.org/papers/v11/vincent10a.html},
	number = {Dec},
	urldate = {2017-10-04},
	journal = {Journal of Machine Learning Research},
	author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	year = {2010},
	pages = {3371--3408},
	file = {Full Text PDF:C\:\\Users\\Veblen\\Zotero\\storage\\R3FGJKIF\\Vincent et al. - 2010 - Stacked Denoising Autoencoders Learning Useful Re.pdf:application/pdf;Snapshot:C\:\\Users\\Veblen\\Zotero\\storage\\TAMEI5AC\\vincent10a.html:text/html}
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	copyright = {© 1986 Nature Publishing Group},
	issn = {0028-0836},
	url = {http://www.nature.com/nature/journal/v323/n6088/abs/323533a0.html?foxtrotcallback=true},
	doi = {10.1038/323533a0},
	language = {en},
	number = {6088},
	urldate = {2017-10-05},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	pages = {533--536},
	file = {Full Text PDF:C\:\\Users\\Veblen\\Zotero\\storage\\WMRQ4WMF\\Rumelhart et al. - 1986 - Learning representations by back-propagating error.pdf:application/pdf;Snapshot:C\:\\Users\\Veblen\\Zotero\\storage\\79VGDICE\\323533a0.html:text/html}
}

@article{hinton_reducing_2006,
	title = {Reducing the {Dimensionality} of {Data} with {Neural} {Networks}},
	volume = {313},
	copyright = {American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/313/5786/504},
	doi = {10.1126/science.1127647},
	abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.
Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.
Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.},
	language = {en},
	number = {5786},
	urldate = {2017-10-06},
	journal = {Science},
	author = {Hinton, G. E. and Salakhutdinov, R. R.},
	month = jul,
	year = {2006},
	pmid = {16873662},
	pages = {504--507},
	file = {Full Text PDF:C\:\\Users\\Veblen\\Zotero\\storage\\NWGYJU2B\\Hinton and Salakhutdinov - 2006 - Reducing the Dimensionality of Data with Neural Ne.pdf:application/pdf;Snapshot:C\:\\Users\\Veblen\\Zotero\\storage\\32DASDNR\\504.html:text/html}
}

@techreport{rumelhart_learning_1985,
	title = {Learning {Internal} {Representations} by {Error} {Propagation}},
	url = {http://www.dtic.mil/docs/citations/ADA164453},
	abstract = {This paper presents a generalization of the perception learning procedure for learning the correct sets of connections for arbitrary networks. The rule, falled the generalized delta rule, is a simple scheme for implementing a gradient descent method for finding weights that minimize the sum squared error of the sytem's performance. The major theoretical contribution of the work is the procedure called error propagation, whereby the gradient can be determined by individual units of the network based only on locally available information. The major empirical contribution of the work is to show that the problem of local minima not serious in this application of gradient descent. Keywords: Learning; networks; Perceptrons; Adaptive systems; Learning machines; and Back propagation.},
	number = {ICS-8506},
	urldate = {2017-11-12},
	institution = {CALIFORNIA UNIV SAN DIEGO LA JOLLA INST FOR COGNITIVE SCIENCE, CALIFORNIA UNIV SAN DIEGO LA JOLLA INST FOR COGNITIVE SCIENCE},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = sep,
	year = {1985},
	file = {Full Text PDF:C\:\\Users\\Veblen\\Zotero\\storage\\PI9M9RIV\\Rumelhart et al. - 1985 - Learning Internal Representations by Error Propaga.pdf:application/pdf;Snapshot:C\:\\Users\\Veblen\\Zotero\\storage\\PGYGEM2T\\ADA164453.html:text/html}
}

@misc{noauthor_mnist_nodate,
	title = {{MNIST} handwritten digit database, {Yann} {LeCun}, {Corinna} {Cortes} and {Chris} {Burges}},
	url = {http://yann.lecun.com/exdb/mnist/},
	urldate = {2017-11-12},
	file = {MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges:C\:\\Users\\Veblen\\Zotero\\storage\\FY5MWAIQ\\mnist.html:text/html}
}

@inproceedings{baldi2012autoencoders,
  title={Autoencoders, unsupervised learning, and deep architectures},
  author={Baldi, Pierre},
  booktitle={Proceedings of ICML Workshop on Unsupervised and Transfer Learning},
  pages={37--49},
  year={2012}
}

@article{leou11comparison,
  title={COMPARISON OF LEARNING ALGORITHMS FOR HANDWRITTEN DIGIT RECOGNITION},
  author={LeOu11, Y and Jackal, L and Bottou, L and Brunet, A and Cortes, C and Denker, J and Drucker, H and Guyon, I and Miiller, U and S{\'e}ckinger, E and others}
}