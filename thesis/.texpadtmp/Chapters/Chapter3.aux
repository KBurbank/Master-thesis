\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Network Architectures and Training Processes}{7}{chapter.16}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Chapter 3}{{3}{7}{Network Architectures and Training Processes}{chapter.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Network Architectures}{7}{section.17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}PanNet and BackPanNet}{7}{subsection.18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}LeNet}{8}{subsection.19}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces [Replace all this with a simple description of what you are seeing. Label the figure with the names of the layers.] Network architectures: all networks adopt the convolutional neural network architecture. PanNet(-enlarged) and backPanNet(-enlarged) use same padding algorithm but LeNet-1(-enlarged) uses valid padding algorithm, which means no padding. As a result, the outputs of LeNet-1(-enlarged) from \(1^{st}\) and \(2^{nd}\) convolution are of dimension \(24 \times 24\) and \(8 \times 8\) respectively instead. Pooling algorithm is \(2 \times 2\) max-pooling. Number of filters for the first convolutional layer \(Nf_{1}\) and the one for the second convolutional layer \(Nf_{2}\) are (4, 12) for regular networks and (120, 150) for enlarged networks. \relax }}{8}{figure.caption.20}}
\newlabel{fig:network architecture}{{3.1}{8}{[Replace all this with a simple description of what you are seeing. Label the figure with the names of the layers.] Network architectures: all networks adopt the convolutional neural network architecture. PanNet(-enlarged) and backPanNet(-enlarged) use same padding algorithm but LeNet-1(-enlarged) uses valid padding algorithm, which means no padding. As a result, the outputs of LeNet-1(-enlarged) from \(1^{st}\) and \(2^{nd}\) convolution are of dimension \(24 \times 24\) and \(8 \times 8\) respectively instead. Pooling algorithm is \(2 \times 2\) max-pooling. Number of filters for the first convolutional layer \(Nf_{1}\) and the one for the second convolutional layer \(Nf_{2}\) are (4, 12) for regular networks and (120, 150) for enlarged networks. \relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Training}{8}{section.21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Training data}{8}{subsection.22}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A sample MNIST image: MNIST images have a height and width of 28 pixels with each pixel indicating the gray level. The value of each pixel is between 0 and 1.\relax }}{9}{figure.caption.23}}
\newlabel{fig:input}{{3.2}{9}{A sample MNIST image: MNIST images have a height and width of 28 pixels with each pixel indicating the gray level. The value of each pixel is between 0 and 1.\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Training algorithms}{9}{subsection.24}}
\@writefile{toc}{\contentsline {subsubsection}{PanNet and PanNet-enlarged}{9}{section*.25}}
\@writefile{toc}{\contentsline {subsubsection}{LeNet}{9}{section*.26}}
\@writefile{toc}{\contentsline {subsubsection}{BackPanNet}{9}{section*.28}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Training process for PanNet(-enlarged): the first convolutional layer is firstly trained aiming to minimize autoencoder loss function, with the goal to reproduce the input information. Weights and biases for the first convolutional layer are fixed after the first training process. Then the second convolutional layer is trained using autoencoder loss function, with the goal to reproduce information entering the second convolutional layer. Weight and biases for the second convolutional layer are fixed after the second training process. Finally, the third layer is trained with cross entropy loss function using the true labels.\relax }}{10}{figure.caption.27}}
\newlabel{fig:PanNet-training}{{3.3}{10}{Training process for PanNet(-enlarged): the first convolutional layer is firstly trained aiming to minimize autoencoder loss function, with the goal to reproduce the input information. Weights and biases for the first convolutional layer are fixed after the first training process. Then the second convolutional layer is trained using autoencoder loss function, with the goal to reproduce information entering the second convolutional layer. Weight and biases for the second convolutional layer are fixed after the second training process. Finally, the third layer is trained with cross entropy loss function using the true labels.\relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Training Process for LeNet-1 and LeNet-1-enlarged}{10}{subsection.30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Training Process for backPanNet and backPanNet-enlarged}{10}{subsection.32}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Hyperparameter Selection}{10}{section.34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Hyperparameter Selection for PanNet}{10}{subsection.35}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Training process for PanNet(-enlarged) - the first training step: this network is trained aiming to enable outputs reproduce inputs. After the first training step, encoder weights and biases are used in the original network.\relax }}{11}{figure.caption.29}}
\newlabel{fig:PanNet_first_training}{{3.4}{11}{Training process for PanNet(-enlarged) - the first training step: this network is trained aiming to enable outputs reproduce inputs. After the first training step, encoder weights and biases are used in the original network.\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Training Process for LeNet-1(-enlarged)\relax }}{11}{figure.caption.31}}
\newlabel{fig:LeNet-1-training}{{3.5}{11}{Training Process for LeNet-1(-enlarged)\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Hyperparameter Selection for PanNet-enlarged}{11}{subsection.39}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Training Process for backPanNet(-enlarged): Firstly, backPanNet(-enlarged) is trained using autoencoder loss functions for the first two convolutional layers. Weights and biases of the first convolutional layer are pre-fixed during the training step for the second convolutional layer. Using the results from stacked convolutional autoencoder training process as initial values for the first two convolutional layers, the network is then trained using cross entropy with true labels and error backpropagation among the whole network.\relax }}{12}{figure.caption.33}}
\newlabel{fig:backPanNet-training}{{3.6}{12}{Training Process for backPanNet(-enlarged): Firstly, backPanNet(-enlarged) is trained using autoencoder loss functions for the first two convolutional layers. Weights and biases of the first convolutional layer are pre-fixed during the training step for the second convolutional layer. Using the results from stacked convolutional autoencoder training process as initial values for the first two convolutional layers, the network is then trained using cross entropy with true labels and error backpropagation among the whole network.\relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Summary}{12}{section.41}}
\newlabel{RF1}{13}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Summary of all neural networks\relax }}{13}{table.caption.42}}
\newlabel{tab:summary}{{3.1}{13}{Summary of all neural networks\relax }{table.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces PanNet  First autoencoder loss versus number of training batches\relax }}{14}{figure.caption.36}}
\newlabel{fig:PanNet_search_parameters_1}{{3.7}{14}{PanNet\\ First autoencoder loss versus number of training batches\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces PanNet  Second autoencoder loss versus number of training batches\relax }}{14}{figure.caption.37}}
\newlabel{fig:PanNet_search_parameters_2}{{3.8}{14}{PanNet\\ Second autoencoder loss versus number of training batches\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces PanNet  Final cross entropy loss and accuracy versus number of training batches\relax }}{14}{figure.caption.38}}
\newlabel{fig:PanNet_search_parameters_3}{{3.9}{14}{PanNet\\ Final cross entropy loss and accuracy versus number of training batches\relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Hyperparameter Selection for PanNet-enlarged\relax }}{15}{figure.caption.40}}
\newlabel{fig:PanNet-enlarge_selection}{{3.10}{15}{Hyperparameter Selection for PanNet-enlarged\relax }{figure.caption.40}{}}
\@setckpt{Chapters/Chapter3}{
\setcounter{page}{16}
\setcounter{equation}{0}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{4}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{10}
\setcounter{table}{1}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{parentequation}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{16}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextrayear}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{cbx@tempcnta}{0}
\setcounter{cbx@tempcntb}{8}
\setcounter{nlinenum}{0}
\setcounter{r@tfl@t}{1}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{26}
\setcounter{section@level}{1}
}
